{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gGVhV2kHQg0"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNBWUEQ-HQg1"
   },
   "source": [
    "# Convolutional Neural Networks (Prepare)\n",
    "\n",
    "> Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. *Goodfellow, et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UbEmkUqHQg1"
   },
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "- <a href=\"#p3\">Part 3: </a>Use a pre-trained convolution neural network for image classification\n",
    "\n",
    "Modern __computer vision__ approaches rely heavily on convolutions as both a dimensionality reduction and feature extraction method. Before we dive into convolutions, let's talk about some of the common computer vision applications: \n",
    "* Classification [(Hot Dog or Not Dog)](https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "* Object Detection [(YOLO)](https://www.youtube.com/watch?v=MPU2HistivI)\n",
    "* Pose Estimation [(PoseNet)](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n",
    "* Facial Recognition [Emotion Detection](https://www.cbronline.com/wp-content/uploads/2018/05/Mona-lIsa-test-570x300.jpg)\n",
    "* and *countless* more \n",
    "\n",
    "We are going to focus on classification and pre-trained classification today. What are some of the applications of image classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "hP6M3V-YHQg2",
    "outputId": "446a957f-8d62-42f6-81c9-1191c1f4bf46"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MPU2HistivI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly00NdnJHQg5",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Convolution & Pooling (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06h_3Bj3HQg6",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Like neural networks themselves, CNNs are inspired by biology - specifically, the receptive fields of the visual cortex.\n",
    "\n",
    "Put roughly, in a real brain the neurons in the visual cortex *specialize* to be receptive to certain regions, shapes, colors, orientations, and other common visual features. In a sense, the very structure of our cognitive system transforms raw visual input, and sends it to neurons that specialize in handling particular subsets of it.\n",
    "\n",
    "CNNs imitate this approach by applying a convolution. A convolution is an operation on two functions that produces a third function, showing how one function modifies another. Convolutions have a [variety of nice mathematical properties](https://en.wikipedia.org/wiki/Convolution#Properties) - commutativity, associativity, distributivity, and more. Applying a convolution effectively transforms the \"shape\" of the input.\n",
    "\n",
    "One common confusion - the term \"convolution\" is used to refer to both the process of computing the third (joint) function and the process of applying it. In our context, it's more useful to think of it as an application, again loosely analogous to the mapping from visual field to receptive areas of the cortex in a real animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "4M5hnthyHQg6",
    "outputId": "0b6e8acc-d158-47fc-9265-0c67bbd73c79"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('IOHayh06LJ4', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw3X-_GsHQg8",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's try to do some convolutions and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krm3RupmHQg9"
   },
   "source": [
    "### Convolution\n",
    "\n",
    "Consider blurring an image - assume the image is represented as a matrix of numbers, where each number corresponds to the color value of a pixel.\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.27.17+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "\n",
    "Helpful Terms:\n",
    "- __Filter__: The weights (parameters) we will apply to our input image.\n",
    "- __Stride__: How the filter moves across the image\n",
    "- __Padding__: Zeros (or other values) around the  the input image border (kind of like a frame of zeros). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsAcbKvoeaqU",
    "outputId": "bac22f49-8e0b-4f50-9cf1-0ea7e2d650b9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color, io\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "austen = io.imread('https://dl.airtable.com/S1InFmIhQBypHBL0BICi_austen.jpg')\n",
    "austen_grayscale = rescale_intensity(color.rgb2gray(austen))\n",
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "KN-ibr_DhyaV",
    "outputId": "2ee35e86-8924-4de0-e9b4-aff8eaa5b514"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QopB0uo6lNxq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the horizontal lines in an image\n",
    "# during lecture I reasoned outloud how this matrix is actually able to preserve the horizontal lines\n",
    "# CHALLENGE: try to convince yourself that you understand how these matrices are able to do what they do  \n",
    "horizontal_edge_convolution = np.array([[1,1,1,1],\n",
    "                                        [0,0,0,0],\n",
    "                                        [0,0,0,0],\n",
    "                                        [-1,-1,-1,-1]])\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the vertical lines in an image \n",
    "vertical_edge_convolution = np.array([[1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1]])\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the diagonal lines in an image \n",
    "diag_edge_convolution = np.array([[1,0,0,-1],\n",
    "                                  [0,1,-1,0],\n",
    "                                  [0,-1,1,0],\n",
    "                                  [-1,0,0,1]])\n",
    "\n",
    "# Doc for nd.convolve: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html\n",
    "austen_edges_vert = nd.convolve(austen_grayscale, vertical_edge_convolution)\n",
    "austen_edges_horz = nd.convolve(austen_grayscale, horizontal_edge_convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "-LwEpFW1l-6b",
    "outputId": "4d6006f1-409f-4d09-a077-5d0cc09da8f6"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges_vert, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "8TyaToNjHQhH",
    "outputId": "9eb3bbde-d847-4c3c-d837-9fc922aaa8df"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges_horz, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDaTY2uSHQhJ"
   },
   "source": [
    "### Pooling Layer\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.26.13+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "We use Pooling Layers to reduce the dimensionality of the feature maps. We get smaller and smaller feature set by apply convolutions and then pooling layers. \n",
    "\n",
    "Let's take a look very simple example using Austen's pic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "eAePA6xCHsn7",
    "outputId": "387d709c-723c-41bc-b54d-68a41f6d1faf"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "PUcpIHm_HQhK",
    "outputId": "c153b7de-1596-4067-c4f2-1b0bdeef64dc"
   },
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce\n",
    "\n",
    "reduced = block_reduce(austen_grayscale, (2,2), np.max)\n",
    "plt.imshow(reduced, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T18SB3PMHQhL",
    "outputId": "919c113f-9f6a-402f-d859-029b6de0e6fd"
   },
   "outputs": [],
   "source": [
    "# by using a MaxPooling of (2,2) we are able to reduce the size of our image by a factor of 2 with out noticably lossing any important information\n",
    "# we still preserve the light contrast on his face\n",
    "# we still preserve the lines on his face\n",
    "# there is a spot on the left side (our left) of his upper mouth that gets a smoothed out a bit \n",
    "reduced.shape, austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0IO7nP9HQhN"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to be able to describe convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qty8NQlnHQhN"
   },
   "source": [
    "# CNNs for Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK6ZUOQZHQhO",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOep4ugw8coa"
   },
   "source": [
    "### Typical CNN Architecture\n",
    "\n",
    "![A Typical CNN](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
    "\n",
    "The first stage of a CNN is, unsurprisingly, a convolution - specifically, a transformation that maps regions of the input image to neurons responsible for receiving them. The convolutional layer can be visualized as follows:\n",
    "\n",
    "![Convolutional layer](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)\n",
    "\n",
    "The red represents the original input image, and the blue the neurons that correspond.\n",
    "\n",
    "As shown in the first image, a CNN can have multiple rounds of convolutions, [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) (a digital signal processing technique that effectively reduces the information by passing through a filter), and then eventually a fully connected neural network and output layer. Typical output layers for a CNN would be oriented towards classification or detection problems - e.g. \"does this picture contain a cat, a dog, or some other animal?\"\n",
    "\n",
    "\n",
    "#### A Convolution in Action\n",
    "\n",
    "![Convolution](https://miro.medium.com/max/1170/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
    "\n",
    "\n",
    "\n",
    "Why are CNNs so popular?\n",
    "1. Compared to prior image learning techniques, they require relatively little image preprocessing (cropping/centering, normalizing, etc.)\n",
    "2. Relatedly, they are *robust* to all sorts of common problems in images (shifts, lighting, etc.)\n",
    "\n",
    "Actually training a cutting edge image classification CNN is nontrivial computationally - the good news is, with transfer learning, we can get one \"off-the-shelf\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_8-zDNHQhP"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWKwH7DIHQhP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential, Model # <- May Use\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6HR5Jz4HQhR"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "o8qu6KtQHQhT",
    "outputId": "23b7d1bb-f237-47c9-a11d-e14b26f7ba88"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Yh04xEGHQhV",
    "outputId": "d53d5afd-5218-4da4-9896-34b9c925321d"
   },
   "outputs": [],
   "source": [
    "# this is a Rank 3 tensor \n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLsZ6GM_IhMV",
    "outputId": "43b38f01-7540-453c-b933-cb463af514e3"
   },
   "outputs": [],
   "source": [
    "# this is a Rank 4 tensor \n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa5ZcbSWHQha",
    "outputId": "34cccff5-803a-4ca1-cd06-d1c48ffb82fd"
   },
   "outputs": [],
   "source": [
    "# Setup Architecture\n",
    "\n",
    "# keras calls them filters and kernals, we call them windows\n",
    "# windows have cell values (i.e. this is a weight matrix)\n",
    "# window cell values are randomly initialized \n",
    "n_windows = 32\n",
    "\n",
    "# specify the window size (i.e. 3 cell high and 3 cell wide)\n",
    "window_size = (3,3)\n",
    "\n",
    "# output of the convolutions between the windows and image will be passed into the activation function (if activate function isn't None)\n",
    "act_func = 'relu'\n",
    "\n",
    "# dim of the image: 32 cell high, 32 cell wide, and 3 channels (one for Red, one for Blue, and one for Green)\n",
    "# this means our images are not matrices, they are tensors \n",
    "image_dim = (32,32,3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### Note about the weights in the convolutional layers ####\n",
    "# during training, the weights in the weight matrix (i.e. the windows used for convolutions) are updated \n",
    "# these weights are responsible for identifying important features in the images (i.e. feature engineering)\n",
    "# they must be tuned so that each convolutional layer is able to identify features (i.e. feature engineering)\n",
    "# each layer creates features, then pass those features to the next layer so that the next layer can use those features to create new features (hieratical features!)\n",
    "\n",
    "\n",
    "# 1st conv layer \n",
    "model.add(Conv2D(n_windows, window_size, activation=act_func, input_shape=image_dim))\n",
    "# 1st pooling layer \n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "# 2st conv layer \n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "# 2st pooling layer \n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "# 3st conv layer \n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "# flatten the image matrix into a row vector \n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer in FCFF portion of model \n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Niy6az_HQhc"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "e5_NmqZHHQhd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20752aa3-2c3d-49be-e9b7-8a83c7f031bd"
   },
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          epochs=10, \n",
    "          validation_data=(test_images, test_labels)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8JXvZjDHQhf",
    "outputId": "ac4f4259-078c-443f-941d-9929a957586a"
   },
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnzHwPbDHQhh"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will apply CNNs to a classification task in the module project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGFiPYCVHQhh"
   },
   "source": [
    "# Transfer Learning for Image Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rCyTJl2HQhh",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic_wzFnprwXI"
   },
   "source": [
    "### Transfer Learning Repositories\n",
    "\n",
    "#### TensorFlow Hub\n",
    "\n",
    "\"A library for reusable machine learning modules\"\n",
    "\n",
    "This lets you quickly take advantage of a model that was trained with thousands of GPU hours. It also enables transfer learning - reusing a part of a trained model (called a module) that includes weights and assets, but also training the overall model some yourself with your own data. The advantages are fairly clear - you can use less training data, have faster training, and have a model that generalizes better.\n",
    "\n",
    "https://www.tensorflow.org/hub/\n",
    "\n",
    "TensorFlow Hub is very bleeding edge, and while there's a good amount of documentation out there, it's not always updated or consistent. You'll have to use your problem-solving skills if you want to use it!\n",
    "\n",
    "#### Keras API - Applications\n",
    "\n",
    "> Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "\n",
    "There is a decent selection of important benchmark models. We'll focus on an image classifier: ResNet50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhF4z1aSHQhi"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM_ApKbGYM9S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def process_img_path(img_path):\n",
    "  \"\"\"\n",
    "  Using tensorflow per-build image processor. \n",
    "\n",
    "  Returns processed image. \n",
    "  \"\"\"\n",
    "  return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "def img_contains_banana(img):\n",
    "  \"\"\"\n",
    "  Imputs image into resnet50 pre-trained model and returns the top 3 likely labels for the image (ranked by largest probability)\n",
    "  \"\"\"\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  model = ResNet50(weights='imagenet')\n",
    "  features = model.predict(x)\n",
    "  results = decode_predictions(features, top=3)[0]\n",
    "  print(results)\n",
    "  for entry in results:\n",
    "    if entry[1] == 'banana':\n",
    "      return entry[2]\n",
    "  return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cQ8ZsJF_Z3B"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "image_urls = [\"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/negative_examples/example11.jpeg\",\n",
    "              \"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/positive_examples/example0.jpeg\"]\n",
    "\n",
    "for _id,img in enumerate(image_urls): \n",
    "    r = requests.get(img)\n",
    "    with open(f'example{_id}.jpg', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Gxzkai0q_d-4",
    "outputId": "5ed8f74d-b559-4393-838c-b50051eacd87"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./example0.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8NIlClb_n8s",
    "outputId": "ae4c88ef-c094-4b2e-dc8f-42c48a111bf9"
   },
   "outputs": [],
   "source": [
    "processed_imaged = process_img_path('example0.jpg')\n",
    "\n",
    "img_contains_banana(processed_imaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "YIwtRazQ_tQr",
    "outputId": "e7d9e67b-ff66-4635-f1a7-7aad2f400d63"
   },
   "outputs": [],
   "source": [
    "Image(filename='example1.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDXwkPWOAB14",
    "outputId": "0274b6c8-8b05-4d6d-fcb8-ea48e787f342"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdF5A88oPYvX"
   },
   "source": [
    "Notice that, while it gets it right, the confidence for the banana image is fairly low. That's because so much of the image is \"not-banana\"! How can this be improved? Bounding boxes to center on items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTAcXO7THQht"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to apply a pretrained model to a classificaiton problem today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCSImm1cHQhu"
   },
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "    * A Convolution is a function applied to another function to produce a third function\n",
    "    * Convolutional Kernels are typically 'learned' during the process of training a Convolution Neural Network\n",
    "    * Pooling is a dimensionality reduction technique that uses either Max or Average of a feature map region to downsample data\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "    * Keras has layers for convolutions :) \n",
    "- <a href=\"#p3\">Part 3: </a>Transfer Learning for Image Classification\n",
    "    * Check out both pretinaed models available in Keras & TensorFlow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYEX1etrHQhu"
   },
   "source": [
    "# Sources\n",
    "\n",
    "- *_Deep Learning_*. Goodfellow *et al.*\n",
    "- *Hands-on Machine Learnign with Scikit-Learn, Keras & Tensorflow*\n",
    "- [Keras CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "- [Tensorflow + Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
    "- [Convolution Wiki](https://en.wikipedia.org/wiki/Convolution)\n",
    "- [Keras Conv2D: Working with CNN 2D Convolutions in Keras](https://missinglink.ai/guides/keras/keras-conv2d-working-cnn-2d-convolutions-keras/)\n",
    "- [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "- [A Beginner's Guide to Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHhtQf8Fr27W"
   },
   "source": [
    "------\n",
    "# Referenced used duirng lecutre \n",
    "\n",
    "[Image of Tensors](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB), we learned that vectors and Matricies are special cases of a more general data structure called a Tensor. \n",
    "\n",
    "[CNN Feature Engineering](https://qjjnh3a9hpo1nukrg1fwoh71-wpengine.netdna-ssl.com/wp-content/uploads/2019/07/1_ZD3ewOfpfsMAjhp4MYFnog-edited.jpg), we learned that each convolutional layer is creating features from the images that are passed into it. This image shows how a CNN learns edges, object parts, and then the entire object as learned features from previous convolutional layers are passed to sunsequent convolutional layers. \n",
    "\n",
    "[Stanford University CNN class](https://cs231n.github.io/convolutional-networks/), in lecture I refered the annimation at about half way down the page that shows how the filter matrices (i.e. the weight matrices) are overlapped over the Red, Yellow, and Blue channels of a color image in order to calculate convolutions and how the results are stored in an output volume. \n",
    "\n",
    "[Py Image Search](https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/#:~:text=increase%20when%20necessary.-,kernel_size,(7%2C%207)%20tuples.&text=%2C%20a%202%2Dtuple%20specifying%20the,of%20the%202D%20convolution%20window.), this link provdies an in-depth exploration of each of the Conv2D parameters and how to select specific values for each parameter. \n",
    "\n",
    "[Keras Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) and of course don't forget about the documention for the python package that we are using to build our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-jtLC05r8Y_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LS_DS_432_Convolutional_Neural_Networks_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
